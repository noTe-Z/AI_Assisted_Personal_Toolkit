<body>
    <div id="header">
        <div id="header_left">
            <a href="http://cvpr2024.thecvf.com/"><img src="/img/cvpr2024_logo.svg" width="175" border="0" alt="CVPR 2024"></a>
            <a href="https://www.thecvf.com/"><img src="/img/cropped-cvf-s.jpg" width="175" height="112" border="0" alt="CVF"></a>
        </div>
        <div id="header_right">
            <div id="header_title">
                <a href="http://cvpr2024.thecvf.com/">CVPR 2024</a> <a href="/menu" class="a_monochrome">open access</a>
            </div>
            <div id="help">
                These CVPR 2024 papers are the Open Access versions, provided by the
    <a href="https://www.thecvf.com/">Computer Vision Foundation.</a><br>
    Except for the watermark, they are identical to the accepted versions;
    the final published version of the proceedings is available on IEEE Xplore.
            </div>
            <div id="disclaimer">
                This material is presented to ensure timely dissemination of scholarly and technical work.
    Copyright and all rights therein are retained by authors or by other copyright holders.
    All persons copying this information are expected to adhere to the terms and constraints invoked 
    by each author's copyright.<br><br>
                <form action="/CVPR2024" method="post">
                    <input type="text" name="query">
                    <input type="submit" value="Search">
                </form>
    
            </div>
        </div>
        <div id="header_sponsor">
            <p style="vertical-align:center; text-align: center"> <strong>Powered by:</strong></p>
            <img src="/img/ms-azure-logo.png" width="100" alt="Microsoft Azure">
            <p> </p>
            <p> </p>
            <p style="vertical-align:center; text-align: center"> <strong>Sponsored by:</strong></p>
            <img src="/img/amazon-logo.png" width="100" alt="Amazon">
            <img src="/img/facebook_logo.jpg" width="100" alt="Facebook">
            <img src="/img/Google_2015_logo.svg" width="100" alt="Google">
        </div>
    </div>
    <div class="clear"></div>
    <div id="content">    <dl>
        <div id="papertitle">
            Unmixing Diffusion for Self-Supervised Hyperspectral Image Denoising
        <dd>
        </dd></div>
        <div id="authors">        <br><b><i>Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen, Hiep Luong, Wilfried Philips</i></b>; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 27820-27830
        </div>
        <font size="5">
        <br><b>Abstract</b>
        </font>
        <br><br>
        <div id="abstract">
        Hyperspectral images (HSIs) have extensive applications in various fields such as medicine agriculture and industry. Nevertheless acquiring high signal-to-noise ratio HSI poses a challenge due to narrow-band spectral filtering. Consequently the importance of HSI denoising is substantial especially for snapshot hyperspectral imaging technology. While most previous HSI denoising methods are supervised creating supervised training datasets for the diverse scenes hyperspectral cameras and scan parameters is impractical. In this work we present Diff-Unmix a self-supervised denoising method for HSI using diffusion denoising generative models. Specifically Diff-Unmix addresses the challenge of recovering noise-degraded HSI through a fusion of Spectral Unmixing and conditional abundance generation. Firstly it employs a learnable block-based spectral unmixing strategy complemented by a pure transformer-based backbone. Then we introduce a self-supervised generative diffusion network to enhance abundance maps from the spectral unmixing block. This network reconstructs noise-free Unmixing probability distributions effectively mitigating noise-induced degradations within these components. Finally the reconstructed HSI is reconstructed through unmixing reconstruction by blending the diffusion-adjusted abundance map with the spectral endmembers. Experimental results on both simulated and real-world noisy datasets show that Diff-Unmix achieves state-of-the-art performance.
        </div>
        <font size="5">
        <br><b>Related Material</b>
        </font>
        <br><br>    <dd>
    [<a href="/content/CVPR2024/papers/Zeng_Unmixing_Diffusion_for_Self-Supervised_Hyperspectral_Image_Denoising_CVPR_2024_paper.pdf">pdf</a>]
    [<a href="/content/CVPR2024/supplemental/Zeng_Unmixing_Diffusion_for_CVPR_2024_supplemental.pdf">supp</a>]
    <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
    <div class="bibref pre-white-space">@InProceedings{Zeng_2024_CVPR,
        author    = {Zeng, Haijin and Cao, Jiezhang and Zhang, Kai and Chen, Yongyong and Luong, Hiep and Philips, Wilfried},
        title     = {Unmixing Diffusion for Self-Supervised Hyperspectral Image Denoising},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        month     = {June},
        year      = {2024},
        pages     = {27820-27830}
    }</div>
    </div>        
    </dd></dl></div>
    
    </body>